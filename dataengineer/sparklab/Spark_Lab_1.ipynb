{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "542e4f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host=redshift.amazonaws.com dbname=dwh user=dwhuser password=Passw0rd port=5439\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('/Users/makbulhussain/Downloads/dwh.cfg')\n",
    "print(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099eb978",
   "metadata": {},
   "source": [
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f7e7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "if 'sc' in locals():\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88b35fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = pyspark.SparkContext(appName=\"maps_and_lazy_evaluation_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24fae18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext\n",
    "\n",
    "# sc = SparkContext(\"local[*]\", \"my_spark_app\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b38ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "songtitle\n"
     ]
    }
   ],
   "source": [
    "log_of_songs = [\n",
    "        \"Despacito\",\n",
    "        \"Nice for what\",\n",
    "        \"No tears left to cry\",\n",
    "        \"Despacito\",\n",
    "        \"Havana\",\n",
    "        \"In my feelings\",\n",
    "        \"Nice for what\",\n",
    "        \"despacito\",\n",
    "        \"All the stars\"\n",
    "]\n",
    "\n",
    "# parallelize the log_of_songs to use with Spark\n",
    "\n",
    "\n",
    "# show the original input data is preserved\n",
    "\n",
    "\n",
    "# create a python function to convert strings to lowercase\n",
    "def convert_song_to_lowercase(song):\n",
    "    return song.lower()\n",
    "\n",
    "print(convert_song_to_lowercase(\"Songtitle\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b67a0e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/09 09:41:53 WARN Utils: Your hostname, Makbuls-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.10.30.219 instead (on interface en0)\n",
      "23/08/09 09:41:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/09 09:41:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Nice for what                                                       (0 + 8) / 8]\n",
      "despacito\n",
      "All the stars\n",
      "Havana\n",
      "Nice for what\n",
      "No tears left to cry\n",
      "In my feelings\n",
      "Despacito\n",
      "Despacito\n",
      "despacito                                                                       \n",
      "all the stars\n",
      "despacito\n",
      "despacito\n",
      "havana\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "havana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no tears left to cry\n",
      "in my feelings\n",
      "nice for what\n",
      "nice for what\n",
      "Despacito\n",
      "Nice for what\n",
      "No tears left to cry\n",
      "In my feelings\n",
      "despacito\n",
      "All the stars\n",
      "Havana\n",
      "Nice for what\n",
      "Despacito\n",
      "despacito\n",
      "all the stars\n",
      "nice for what\n",
      "in my feelings\n",
      "despacito\n",
      "nice for what\n",
      "havana\n",
      "no tears left to cry\n",
      "despacito\n",
      "nice for what\n",
      "despacito\n",
      "no tears left to cry\n",
      "in my feelings\n",
      "despacito\n",
      "all the stars\n",
      "nice for what\n",
      "havana\n",
      "despacito\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Because we aren't running on a spark cluster, the session is just for development\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Maps and Lazy Evaluation Example 1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Starting off with a regular python list\n",
    "log_of_songs = [\n",
    "        \"Despacito\",\n",
    "        \"Nice for what\",\n",
    "        \"No tears left to cry\",\n",
    "        \"Despacito\",\n",
    "        \"Havana\",\n",
    "        \"In my feelings\",\n",
    "        \"Nice for what\",\n",
    "        \"despacito\",\n",
    "        \"All the stars\"\n",
    "]\n",
    "\n",
    "# parallelize the log_of_songs to use with Spark\n",
    "# distributed_song_log_rdd is an RDD (Reslient Distributed Dataset)\n",
    "distributed_song_log_rdd = spark.sparkContext.parallelize(log_of_songs)\n",
    "\n",
    "# notice we DO NOT use the .collect() method. What is the difference between\n",
    "# .collect() and .foreach() ? \n",
    "# .collect() forces all the data from the entire RDD on all nodes \n",
    "# to be collected from ALL the nodes, which kills productivity, and could crash\n",
    "# .foreach() allows the data to stay on each of the independent nodes\n",
    "\n",
    "# show the original input data is preserved\n",
    "distributed_song_log_rdd.foreach(print)\n",
    "\n",
    "def convert_song_to_lowercase(song):\n",
    "    return song.lower()\n",
    "\n",
    "print(convert_song_to_lowercase(\"Havana\"))\n",
    "\n",
    "\n",
    "# toDF() Converts from an RDD to a DataFrame- this allows us to use convenient functions like show() \n",
    "lower_case_songs=distributed_song_log_rdd.map(convert_song_to_lowercase)\n",
    "lower_case_songs.foreach(print)\n",
    "\n",
    "# Show the original input data is still mixed case\n",
    "distributed_song_log_rdd.foreach(print)\n",
    "\n",
    "# Use lambda functions instead of named functions to do the same map operation\n",
    "distributed_song_log_rdd.map(lambda song: song.lower()).foreach(print)\n",
    "distributed_song_log_rdd.map(lambda x: x.lower()).foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5b0d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After some initialization, we'll convert the log of songs (just a normal Python list) \n",
    "# to a distributed dataset that Spark can use. This uses a special spark.sparkContext object. \n",
    "# The Spark Context has a method parallelize that takes a Python object and distributes the object across the machines in your cluster so Spark can process the dataset.\n",
    "\n",
    "\n",
    "distributed_song_log_rdd = spark.sparkContext.parallelize(log_of_songs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726e7a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_song_log_rdd.map(convert_song_to_lowercase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f75b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despacito',\n",
       " 'nice for what',\n",
       " 'no tears left to cry',\n",
       " 'despacito',\n",
       " 'havana',\n",
       " 'in my feelings',\n",
       " 'nice for what',\n",
       " 'despacito',\n",
       " 'all the stars']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to force Spark to take some action on the data, \n",
    "# we can use the collect function, which gathers the results from all of the machines in our cluster.\n",
    "distributed_song_log_rdd.map(convert_song_to_lowercase).collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5aa6e2",
   "metadata": {},
   "source": [
    "## Data Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f496632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.driver.host', '10.10.30.219'), ('spark.app.startTime', '1691554314282'), ('spark.app.name', 'Maps and Lazy Evaluation Example 1'), ('spark.executor.id', 'driver'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.driver.port', '58598'), ('spark.app.submitTime', '1691554314111'), ('spark.rdd.compress', 'True'), ('spark.sql.warehouse.dir', 'file:/Users/makbulhussain/spark-warehouse'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.app.id', 'local-1691554315315')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Because we aren't running on a spark cluster, the session is just for development\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Our first Python Spark SQL example\") \\\n",
    "    .getOrCreate()\n",
    "# This should print the default configuration\n",
    "print(\n",
    "    spark.sparkContext.getConf().getAll()\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de15007e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This path resides on your computer or workspace, not in HDFS\n",
    "path = \"/Users/makbulhussain/Downloads/sparkify_log_small.json\"\n",
    "user_log_df = spark.read.json(path)\n",
    "\n",
    "# See how Spark inferred the schema from the JSON file\n",
    "user_log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1641801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[summary: string, artist: string, auth: string, firstName: string, gender: string, itemInSession: string, lastName: string, length: string, level: string, location: string, method: string, page: string, registration: string, sessionId: string, song: string, status: string, ts: string, userAgent: string, userId: string]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    user_log_df.describe()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa2faf98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|Showaddywaddy|Logged In|  Kenneth|     M|          112|Matthews|232.93342| paid|Charlotte-Concord...|   PUT|NextSong|1509380319284|     5132|Christmas Tears W...|   200|1513720872284|\"Mozilla/5.0 (Win...|  1046|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log_df.show(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91c3ee6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046')]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    user_log_df.take(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68ff1546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: string (nullable = true)\n",
      " |-- sessionId: string (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- ts: string (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We are changing file formats\n",
    "out_path = \"/Users/makbulhussain/Downloads/sparkify_log_small.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# The filename alone didn't tell Spark the actual format, we need to do it here\n",
    "user_log_df.write.mode(\"overwrite\").save(out_path, format=\"csv\", header=True)\n",
    "\n",
    "# Notice we have created another dataframe here\n",
    "# We wouldn't usually read the data that we just wrote\n",
    "# This does show, however, that the read method works with\n",
    "# Different data types\n",
    "user_log_2_df = spark.read.csv(out_path, header=True)\n",
    "user_log_2_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c17f45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession='112', lastName='Matthews', length='232.93342', level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration='1509380319284', sessionId='5132', song='Christmas Tears Will Fall', status='200', ts='1513720872284', userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046'), Row(artist='Lily Allen', auth='Logged In', firstName='Elizabeth', gender='F', itemInSession='7', lastName='Chase', length='195.23873', level='free', location='Shreveport-Bossier City, LA', method='PUT', page='NextSong', registration='1512718541284', sessionId='5027', song='Cheryl Tweedy', status='200', ts='1513720878284', userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='1000')]\n"
     ]
    }
   ],
   "source": [
    "# Choose two records from the CSV file\n",
    "print(\n",
    "    user_log_2_df.take(2)\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02837ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userID|\n",
      "+------+\n",
      "|  1046|\n",
      "|  1000|\n",
      "|  2219|\n",
      "+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the userID column for the first several rows\n",
    "user_log_2_df.select(\"userID\").show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95b4860f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession='112', lastName='Matthews', length='232.93342', level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration='1509380319284', sessionId='5132', song='Christmas Tears Will Fall', status='200', ts='1513720872284', userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046')]\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "print(\n",
    "user_log_2_df.take(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9542ba26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|  superartist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|Showaddywaddy|Logged In|  Kenneth|     M|          112|Matthews|232.93342| paid|Charlotte-Concord...|   PUT|NextSong|1509380319284|     5132|Christmas Tears W...|   200|1513720872284|\"Mozilla/5.0 (Win...|  1046|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple Rename\n",
    "user_log_2_df.withColumnRenamed('artist','superartist').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f92d105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+------------+\n",
      "|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|doublelength|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+------------+\n",
      "|Showaddywaddy|Logged In|  Kenneth|     M|          112|Matthews|232.93342| paid|Charlotte-Concord...|   PUT|NextSong|1509380319284|     5132|Christmas Tears W...|   200|1513720872284|\"Mozilla/5.0 (Win...|  1046|   465.86684|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Double the length\n",
    "user_log_2_df.withColumn('doublelength',user_log_2_df['length']*2).show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da95feb0",
   "metadata": {},
   "source": [
    "## Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d84fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "user_log_2_df.createOrReplaceTempView(\"people\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9565b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_results = spark.sql(\"SELECT * FROM people\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2712c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: string, lastName: string, length: string, level: string, location: string, method: string, page: string, registration: string, sessionId: string, song: string, status: string, ts: string, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66e8f1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|Showaddywaddy|Logged In|  Kenneth|     M|          112|Matthews|232.93342| paid|Charlotte-Concord...|   PUT|NextSong|1509380319284|     5132|Christmas Tears W...|   200|1513720872284|\"Mozilla/5.0 (Win...|  1046|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_results.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3e8e51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|Showaddywaddy|Logged In|  Kenneth|     M|          112|Matthews|232.93342| paid|Charlotte-Concord...|   PUT|NextSong|1509380319284|     5132|Christmas Tears W...|   200|1513720872284|\"Mozilla/5.0 (Win...|  1046|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM people WHERE artist='Showaddywaddy'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1996daf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
